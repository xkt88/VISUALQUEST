{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "811d58e6-7491-4bdd-803a-63a5da17897a",
   "metadata": {},
   "source": [
    "# Detection\n",
    "- Constructing Visual pun detection dataset, by separating the text and image, image as input , text as label,dataset size is 3400\n",
    "    - sources: visual pun, visual idiom, emojis, which are recognisable by odinary people\n",
    "    - no matter from which source, the visual pun should be able to recognised by at least one person (let's say from 30 candidates)\n",
    "    - In default case , each image should provide enough cue about the type of information expected, telling whether the image is related to a word? phrase? idiom? proverb? slang? celebrity? movie? famous character?\n",
    "- Testing all the mainstream MLLM's detection abilities (include 4 close-sourced and 4 open-sourced MLLMs)\n",
    "    - one image can have several similar meaning but different wording interprations, like \"blow your mind\" and \" mind blown\" These should be considered the same,\n",
    "    - the MLLM should first generally describe the picture, Then, should try to come up with the answer. (An ablation study can be conducted to prove the necessity of the decription part)\n",
    "    - compare the LLMs performance on different type of images sets, and rank the performance\n",
    "- Why ensembling is the way to go?\n",
    "    - count the total number of at least one LLM is right\n",
    "    - ÊØè‰∏™ÂèÇÂä†ËØÑÊµãÂ§ßÊ®°ÂûãÂÅö‰∏Ä‰∏™6ËæπÂΩ¢ÁöÑÂõæÔºåÁõ¥Êé•ËØÑ‰ª∑ÂêÑÊ®°ÂûãÂú®ÂêÑÊñπÈù¢ÁöÑËÉΩÂäõ,ÂèØ‰ª•ËØÜÂà´ÁöÑÂΩ¢ÂºèÂèØ‰ª•ÂàÜ‰∏∫Ôºö\n",
    "        - Memoji\n",
    "        - Minimalist\n",
    "        - Emoji Combination\n",
    "        - Visual Pun (should use a pun picture,<b>each word should have a direct cue in the picture</b>) and Metaphorical (more abstractive, without direct use of the word,but the <b> number of this kind is a problem</b>.the difference with visual pun is need to understand an act in an abstracive artistic image )\n",
    "        - Descriptive illustration( in this case the Model should be able to \n",
    "        - Non realistic(encompasses abstract, expressionistic, stylized, cubism and other approaches)\n",
    "     - ËÉåÊôØÁü•ËØÜÂèØ‰ª•ÂàÜ‰∏∫(Áî®ÊäòÁé∞ÂõæÔºâÔºö\n",
    "       - literature( Idiom,proverb,phrase or word)\n",
    "       - POP culture( Movie,TV series,cartoon,Anime,Game,APP,songs, real image based)\n",
    "       - fiction (fictional images)\n",
    "       - real person(real images based)\n",
    "    - count each 2 LLMs complementary ratio(number of set difference/number of recognised samples)\n",
    "- <b>Special cases that need to notices:</b>\n",
    "    - count the total number of unique right, and each LLM's percentage (or number) of unique right\n",
    "    - number of all right images,\n",
    "    - number of all wrong images(if label is somewhat wrong,fix it! ),can show some typical cases, that easy for human to recognise but almost impossible for LLMs\n",
    "- <b> Further Analysis</b>\n",
    "   - Find the samples that all the LLMs can't recognise, then make another group of humans look through these images, get their recognition rate\n",
    "   - analyze what special character does these unrecognizable by LLMs images have, try to summarize their common qualities\n",
    "   - <b>If </b> the unrecognizable by LLM pattern is obserable, In the next work we can come up with a dataset that easy for human's but challenging for LLMs.  <b> It'll be a really great breakthrough!</b> . Of course, this can only be build on the assumption that the challenging pattern is obserable and can be summarized.\n",
    "- DATA TABLE\n",
    "    - <b>Image</b>:name of the image file in the folder, eg: 0001.jpg\n",
    "    - <b>Question</b>: the question attached on the image, guess idiom or guess a phrase or other, there might be more then 10 questions\n",
    "    - <b>Knowledge type</b>: is it a movie?word?story? or a person?\n",
    "    - <b>Expression Style</b>: Minimalist? Nonrealisim? Emoji Combination? Memoji?Visual Pun?Illustration?\n",
    "    - <b>Label</b>:the standard label, more like anchor to the image, eg: alice(alice in the wonderland)\n",
    "    - <b>Description</b>:explain why the label is right for the image , which elments in the image make it what it is, if it's and idiom or metaphor, further explain the it. eg. if the image is \"cry wolf\" then should explain it means false alarm \n",
    "    - <b>Culture</b>:except from the English(UK,USA,Canada...) ,it can be Latine, Greek,Norse,Slavic,Arabic,Japanese,Chinese,Indian, Even more rare sourced cultures like native american, Hawaiian,Inuit\n",
    "    - <b>LLMs</b>:during the experiment ,let's first collect the response of each tested LLM as one column, currently, we want to test <b> GPT,Gemini, Claude, Grok,GLM and LLama</b>\n",
    "    - <b>Num of recognition</b>: range from 0 to 5, 0 means non of 5 LLMs recognised the image , 5 means all five recognised\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9a61f4-79d3-41cf-a1bf-8994dfad5bc8",
   "metadata": {},
   "source": [
    "<b>Exploring Distraction in LLMs Through Image Recognition</b>\n",
    "- If we select a set of images that a language model (LLM) correctly identifies when analyzed individually, and then feed these same images into the model simultaneously, the original correct classifications often turn incorrect. This phenomenon could provide a way to quantify and study the model's susceptibility to distraction or its ability to manage attention.\n",
    "- A key variable to investigate is the number of images. At what point does this phenomenon begin to appear? Is there a maximum number of images beyond which the accuracy stabilizes, regardless of how many additional images are introduced?\n",
    "- Another important factor is whether the order of the images in the prompt impacts the results. For example, does the first image receive more attention or priority compared to subsequent ones?\n",
    "- To gain further insight, we could compare different LLMs. For fairness, the comparison should only involve images that each model has correctly identified when evaluated individually.\n",
    "- Additionally, it would be interesting to explore an alternative approach: instead of feeding the images separately in a single prompt, what if the images are merged into a single composite image? Would this reduce distraction and improve recognition accuracy, or would it exacerbate the problem?\n",
    "\n",
    "This approach offers a compelling framework to study the distraction factor in LLMs. While this idea serves as a starting point, a deeper and more sophisticated investigation could form the basis of a detailed research paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "64b6d90e-dc3d-49bc-83e0-4bd52d818ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def image_to_base64(image_path):\n",
    "    try:\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            # Read the image binary data\n",
    "            binary_data = image_file.read()\n",
    "            # Encode binary data to base64\n",
    "            base64_data = base64.b64encode(binary_data)\n",
    "            # Convert bytes to string\n",
    "            base64_string = base64_data.decode('utf-8')\n",
    "            # Create the complete base64 URL\n",
    "            base64_url = f\"data:image/{image_path.split('.')[-1]};base64,{base64_string}\"\n",
    "            return base64_url\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "238e8eb9-f69f-46ad-baf9-8cf5522a98ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This appears to be Pandora from Greek mythology, depicted in the classic black-figure style of ancient Greek pottery art. According to the myth, she was given a box (though it was actually a jar or \"pithos\" in the original Greek) by the gods, which she opened, releasing all the world's evils. The image shows a female figure in a long flowing dress holding up what appears to be the infamous box, with wispy shapes emerging from it, represented in a traditional Ancient Greek artistic style against an orange background with classical columns.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import httpx\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.xty.app/v1\", \n",
    "    api_key=key,\n",
    "    http_client=httpx.Client(\n",
    "        base_url=\"https://api.xty.app/v1\",\n",
    "        follow_redirects=True,\n",
    "    ),\n",
    ")\n",
    "image_path = \"3429.jpg\" \n",
    "completion = client.chat.completions.create(\n",
    "  model=\"claude-3-5-sonnet-20241022\",\n",
    "  messages=[\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "          {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": \"Who is this?\"\n",
    "          },\n",
    "          {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\"url\":image_to_base64(image_path)}\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7446420a-3bab-4e9f-a81e-bfcb1cc838c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! It's nice to meet you. Is there something I can help you with or would you like to chat?\n",
      "11 24\n"
     ]
    }
   ],
   "source": [
    "# Assume openai>=1.0.0\n",
    "from openai import OpenAI\n",
    "\n",
    "# Create an OpenAI client with your deepinfra token and endpoint\n",
    "openai = OpenAI(\n",
    "    api_key=\"G0QudzLa9xY2YYUIKqKV6qgbF7HpLyTf\",\n",
    "    base_url=\"https://api.deepinfra.com/v1/openai\",\n",
    ")\n",
    "\n",
    "chat_completion = openai.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-3.2-90B-Vision-Instruct\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)\n",
    "print(chat_completion.usage.prompt_tokens, chat_completion.usage.completion_tokens)\n",
    "\n",
    "# Hello! It's nice to meet you. Is there something I can help you with, or would you like to chat?\n",
    "# 11 25\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "144a5250-e7b4-4dd2-a28d-7035965dc766",
   "metadata": {},
   "source": [
    "import os\n",
    "import base64\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import requests\n",
    "def process_images(folder_path):\n",
    "    url = \"https://api.ainewserver.com/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": \"x,\n",
    "        \"content-type\": \"application/json\"\n",
    "    }\n",
    "    results = {}  \n",
    "    valid_extensions = ('.jpg', '.jpeg', '.png')\n",
    "    image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(valid_extensions)]\n",
    "    \n",
    "    for img_file in image_files:\n",
    "        key = os.path.splitext(img_file)[0]\n",
    "        img_path = os.path.join(folder_path, img_file)\n",
    "        \n",
    "        try:\n",
    "            # Read and encode image with proper data URI format\n",
    "            with open(img_path, 'rb') as f:\n",
    "                img_base = base64.b64encode(f.read()).decode('utf-8')\n",
    "                # Add the appropriate data URI prefix based on file extension\n",
    "                if img_file.lower().endswith(('.jpg', '.jpeg')):\n",
    "                    img_base = f\"data:image/jpeg;base64,{img_base}\"\n",
    "                elif img_file.lower().endswith('.png'):\n",
    "                    img_base = f\"data:image/png;base64,{img_base}\"\n",
    "            data = {\n",
    "                \"messages\":[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\":[{\"type\": \"image_url\", \"image_url\": {\"url\": img_base}},\n",
    "                                   {\"type\": \"text\", \"text\": \"concisely answer the question on the image\"}]\n",
    "                    }],\n",
    "                \"model\": \"gemini-2.0-flash-exp\"\n",
    "            }\n",
    "         \n",
    "            response = requests.post(url, headers=headers, json=data)\n",
    "            data = json.loads(response.text)\n",
    "            value = data['choices'][0]['message']['content']   \n",
    "            results[key] = value\n",
    "            print(key,'|',value)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_file}: {str(e)}\")\n",
    "            results[key] = \"Error: \" + str(e)\n",
    "    \n",
    "    output_file = os.path.join(folder_path, 'gemini-2.0-flash-exp.json')\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    return output_file\n",
    "\n",
    "# Usage\n",
    "folder_path = \"test\"\n",
    "output_file = process_images(folder_path)\n",
    "print(f\"Results saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d28a57-b931-4408-abce-045c3fee781a",
   "metadata": {},
   "source": [
    "## GPT"
   ]
  },
  {
   "cell_type": "raw",
   "id": "56891746-d07e-4805-a175-f5a7dc08e2f7",
   "metadata": {},
   "source": [
    "import os\n",
    "import base64\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import requests\n",
    "\n",
    "def process_images(folder_path):\n",
    "    client = OpenAI(\n",
    "    base_url=\"https://api.xty.app/v1\", \n",
    "    api_key=\"xxxx\",\n",
    "    http_client=httpx.Client(\n",
    "        base_url=\"https://api.xty.app/v1\",\n",
    "        follow_redirects=True,\n",
    "    ),\n",
    "    )\n",
    "    output_file = os.path.join(folder_path, 'infer.json')\n",
    "\n",
    "    if os.path.exists(output_file):\n",
    "        with open(output_file, 'r', encoding='utf-8') as f:\n",
    "            results = json.load(f)\n",
    "    else:\n",
    "        results = {}\n",
    "    \n",
    "    valid_extensions = ('.jpg', '.jpeg', '.png')\n",
    "    image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(valid_extensions)]\n",
    "    \n",
    "    for img_file in image_files:\n",
    "        key = os.path.splitext(img_file)[0]\n",
    "        \n",
    "        # Skip if already processed\n",
    "        if key in results:\n",
    "            print(f\"Skipping {img_file} - already processed\")\n",
    "            continue\n",
    "            \n",
    "        img_path = os.path.join(folder_path, img_file)\n",
    "        \n",
    "        try:\n",
    "            # Read and encode image with proper data URI format\n",
    "            with open(img_path, 'rb') as f:\n",
    "                img_base = base64.b64encode(f.read()).decode('utf-8')\n",
    "                # Add the appropriate data URI prefix based on file extension\n",
    "                if img_file.lower().endswith(('.jpg', '.jpeg')):\n",
    "                    img_base = f\"data:image/jpeg;base64,{img_base}\"\n",
    "                elif img_file.lower().endswith('.png'):\n",
    "                    img_base = f\"data:image/png;base64,{img_base}\"\n",
    "                    \n",
    "\n",
    "            completion = client.chat.completions.create(\n",
    "              model=\"gpt-4o-2024-11-20\",\n",
    "              messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\":[{\"type\": \"image_url\", \"image_url\": {\"url\": img_base}},\n",
    "                                            {\"type\": \"text\", \"text\": \"concisely answer the question on the image\"}]}\n",
    "              ]\n",
    "            )\n",
    "            value = completion.choices[0].message.content\n",
    "            results[key] = value\n",
    "            print(key,'|',value)\n",
    "\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_file}: {str(e)}\")\n",
    "            results[key] = \"Error: \" + str(e)\n",
    "            # Save results even if there's an error\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    return output_file\n",
    "\n",
    "# Usage\n",
    "folder_path = \"test\"\n",
    "output_file = process_images(folder_path)\n",
    "print(f\"Results saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea80d95-2bad-4d2b-93a8-cb51c5a4a89f",
   "metadata": {},
   "source": [
    "## Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "78700580-f299-460f-a8a9-761dac923dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm unable to view images directly. However, if you describe an image to me, I can help with analysis, provide information, or answer questions about it! Let me know how I can assist you.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import httpx\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.xty.app/v1\", \n",
    "    api_key=\"xxx\",\n",
    "    http_client=httpx.Client(\n",
    "        base_url=\"https://api.xty.app/v1\",\n",
    "        follow_redirects=True,\n",
    "    ),\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4o\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"can you describe a given image?\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4a8be86e-6892-4a67-afee-f288e670aba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0650 | Tom Hanks\n",
      "Results saved to: a\\infer.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import base64\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import requests\n",
    "\n",
    "def process_images(folder_path):\n",
    "    client = OpenAI(\n",
    "    base_url=\"https://api.xty.app/v1\", \n",
    "    api_key=\"xxx\",\n",
    "    http_client=httpx.Client(\n",
    "        base_url=\"https://api.xty.app/v1\",\n",
    "        follow_redirects=True,\n",
    "    ),\n",
    "    )\n",
    "    output_file = os.path.join(folder_path, 'infer.json')\n",
    "\n",
    "    if os.path.exists(output_file):\n",
    "        with open(output_file, 'r', encoding='utf-8') as f:\n",
    "            results = json.load(f)\n",
    "    else:\n",
    "        results = {}\n",
    "    \n",
    "    valid_extensions = ('.jpg', '.jpeg', '.png')\n",
    "    image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(valid_extensions)]\n",
    "    \n",
    "    for img_file in image_files:\n",
    "        key = os.path.splitext(img_file)[0]\n",
    "        \n",
    "        # Skip if already processed\n",
    "        if key in results:\n",
    "            print(f\"Skipping {img_file} - already processed\")\n",
    "            continue\n",
    "            \n",
    "        img_path = os.path.join(folder_path, img_file)\n",
    "        \n",
    "        try:\n",
    "            # Read and encode image with proper data URI format\n",
    "            with open(img_path, 'rb') as f:\n",
    "                img_base = base64.b64encode(f.read()).decode('utf-8')\n",
    "                # Add the appropriate data URI prefix based on file extension\n",
    "                if img_file.lower().endswith(('.jpg', '.jpeg')):\n",
    "                    img_base = f\"data:image/jpeg;base64,{img_base}\"\n",
    "                elif img_file.lower().endswith('.png'):\n",
    "                    img_base = f\"data:image/png;base64,{img_base}\"\n",
    "                    \n",
    "\n",
    "            completion = client.chat.completions.create(\n",
    "              model=\"gemini-2.0-flash-exp\",\n",
    "              messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\":[{\"type\": \"image_url\", \"image_url\": {\"url\": img_base}},\n",
    "                                            {\"type\": \"text\", \"text\": \"Concisely answer the question on the image!\"}]}\n",
    "              ]\n",
    "            )\n",
    "            value = completion.choices[0].message.content\n",
    "            results[key] = value\n",
    "            print(key,'|',value)\n",
    "\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_file}: {str(e)}\")\n",
    "            results[key] = \"Error: \" + str(e)\n",
    "            # Save results even if there's an error\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    return output_file\n",
    "\n",
    "# Usage\n",
    "folder_path = \"a\"\n",
    "output_file = process_images(folder_path)\n",
    "print(f\"Results saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ef0b9e-2ff6-415c-9f77-96711d9f0704",
   "metadata": {},
   "source": [
    "## Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "78eab6df-7fc4-47a9-97c8-27c23b79b15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001 | I don't see a question in the image - it appears to be a large grid of cells or boxes. Could you please let me know what specific question you'd like me to answer about this image?\n",
      "Error processing 2324.jpg: 'choices'\n",
      "Error processing 2744.jpg: 'choices'\n",
      "3058 | I don't see a specific question in the image. The image appears to be a large block of text or code in a dark color against a light background. Could you please clarify what question you would like me to answer about this image?\n",
      "3429 | I apologize, but I'm unable to see any question in the image. The image appears to be a large table or grid with multiple cells, but I cannot make out any specific question that needs to be answered. Could you please share the question you'd like me to help with?\n",
      "Results saved to: test\\test.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import base64\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import requests\n",
    "\n",
    "def process_images(folder_path):\n",
    "    url = \"https://api.ainewserver.com/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": \"xxx\",\n",
    "        \"content-type\": \"application/json\"\n",
    "    }\n",
    "    output_file = os.path.join(folder_path, 'test.json')\n",
    "    \n",
    "    # Load existing results if file exists\n",
    "    if os.path.exists(output_file):\n",
    "        with open(output_file, 'r', encoding='utf-8') as f:\n",
    "            results = json.load(f)\n",
    "    else:\n",
    "        results = {}\n",
    "    \n",
    "    valid_extensions = ('.jpg', '.jpeg', '.png')\n",
    "    image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(valid_extensions)]\n",
    "    \n",
    "    for img_file in image_files:\n",
    "        key = os.path.splitext(img_file)[0]\n",
    "        \n",
    "        # Skip if already processed\n",
    "        if key in results:\n",
    "            print(f\"Skipping {img_file} - already processed\")\n",
    "            continue\n",
    "            \n",
    "        img_path = os.path.join(folder_path, img_file)\n",
    "        \n",
    "        try:\n",
    "            # Read and encode image with proper data URI format\n",
    "            with open(img_path, 'rb') as f:\n",
    "                img_base = base64.b64encode(f.read()).decode('utf-8')\n",
    "                # Add the appropriate data URI prefix based on file extension\n",
    "                if img_file.lower().endswith(('.jpg', '.jpeg')):\n",
    "                    img_base = f\"data:image/jpeg;base64,{img_base}\"\n",
    "                elif img_file.lower().endswith('.png'):\n",
    "                    img_base = f\"data:image/png;base64,{img_base}\"\n",
    "                    \n",
    "            data = {\n",
    "                \"messages\":[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\":[{\"type\": \"image_url\", \"image_url\": {\"url\": img_base}},\n",
    "                                   {\"type\": \"text\", \"text\": \"concisely answer the question on the image\"}]\n",
    "                    }],\n",
    "                \"model\": \"claude-3-5-sonnet-20241022\"\n",
    "            }\n",
    "         \n",
    "            response = requests.post(url, headers=headers, json=data)\n",
    "            data = json.loads(response.text)\n",
    "            value = data['choices'][0]['message']['content']   \n",
    "            results[key] = value\n",
    "            print(key,'|',value)\n",
    "            \n",
    "            # Save results after each successful processing\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_file}: {str(e)}\")\n",
    "            results[key] = \"Error: \" + str(e)\n",
    "            # Save results even if there's an error\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    return output_file\n",
    "\n",
    "# Usage\n",
    "folder_path = \"test\"\n",
    "output_file = process_images(folder_path)\n",
    "print(f\"Results saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fd283a-bdc3-4d47-aae3-9b27729e6559",
   "metadata": {},
   "source": [
    "## Deepinfra(Llama & Qwen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d9ce01af-3184-442f-acfe-832b77c0c03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0650 | The celebrity in the image is Tom Hanks.\n",
      "Results saved to: a\\view.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import base64\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "def process_images(folder_path):\n",
    "    openai = OpenAI(\n",
    "    api_key=\"G0QudzLa9xY2YYUIKqKV6qgbF7HpLyTf\",\n",
    "    base_url=\"https://api.deepinfra.com/v1/openai\",\n",
    "    )\n",
    "    results = {}  \n",
    "    valid_extensions = ('.jpg', '.jpeg', '.png')\n",
    "    image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(valid_extensions)]\n",
    "    \n",
    "    for img_file in image_files:\n",
    "        key = os.path.splitext(img_file)[0]\n",
    "        img_path = os.path.join(folder_path, img_file)\n",
    "        \n",
    "        try:\n",
    "            # Read and encode image with proper data URI format\n",
    "            with open(img_path, 'rb') as f:\n",
    "                img_base = base64.b64encode(f.read()).decode('utf-8')\n",
    "                # Add the appropriate data URI prefix based on file extension\n",
    "                if img_file.lower().endswith(('.jpg', '.jpeg')):\n",
    "                    img_base = f\"data:image/jpeg;base64,{img_base}\"\n",
    "                elif img_file.lower().endswith('.png'):\n",
    "                    img_base = f\"data:image/png;base64,{img_base}\"\n",
    "            \n",
    "            response = openai.chat.completions.create(\n",
    "                model=\"meta-llama/Llama-3.2-90B-Vision-Instruct\",\n",
    "                messages=[{\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\":[\n",
    "                        {\"type\": \"image_url\", \"image_url\": {\"url\": img_base}},\n",
    "                        {\"type\": \"text\", \"text\": \"concisely answer the question on the image\"}\n",
    "                    ]\n",
    "                }],\n",
    "            )\n",
    "                        \n",
    "            value = response.choices[0].message.content\n",
    "            results[key] = value\n",
    "            print(key,'|',value)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_file}: {str(e)}\")\n",
    "            results[key] = \"Error: \" + str(e)\n",
    "    \n",
    "    output_file = os.path.join(folder_path, 'view.json')\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    return output_file\n",
    "\n",
    "# Usage\n",
    "folder_path = \"a\"\n",
    "output_file = process_images(folder_path)\n",
    "print(f\"Results saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd90ab2-2809-4965-b23b-5b4c975b1348",
   "metadata": {},
   "source": [
    "## GLM"
   ]
  },
  {
   "cell_type": "raw",
   "id": "37e46d0d-53a3-4456-bced-8c5edad0dc2f",
   "metadata": {},
   "source": [
    "import os\n",
    "import base64\n",
    "import json\n",
    "from zhipuai import ZhipuAI\n",
    "\n",
    "def process_images(folder_path, api_key):\n",
    "    # Initialize the client\n",
    "    client = ZhipuAI(api_key=api_key)\n",
    "    \n",
    "    # Dictionary to store results\n",
    "    results = {}\n",
    "    \n",
    "    # Get list of image files\n",
    "    valid_extensions = ('.jpg', '.jpeg', '.png')\n",
    "    image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(valid_extensions)]\n",
    "    \n",
    "    # Process each image\n",
    "    for img_file in image_files:\n",
    "        # Get file name without extension as key\n",
    "        key = os.path.splitext(img_file)[0]\n",
    "        \n",
    "        # Full path to image\n",
    "        img_path = os.path.join(folder_path, img_file)\n",
    "        \n",
    "        try:\n",
    "            # Read and encode image\n",
    "            with open(img_path, 'rb') as f:\n",
    "                img_base = base64.b64encode(f.read()).decode('utf-8')\n",
    "            \n",
    "            # Make API call\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"GLM-4v-plus\",\n",
    "                messages=[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"image_url\", \"image_url\": {\"url\": img_base}},\n",
    "                        {\"type\": \"text\", \"text\": \"concisely answer the question on the image\"}\n",
    "                    ]\n",
    "                }]\n",
    "            )\n",
    "            \n",
    "            # Store result\n",
    "            value = response.choices[0].message.content\n",
    "            results[key] = value\n",
    "            print(key,'|',value)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_file}: {str(e)}\")\n",
    "            results[key] = \"Error: \" + str(e)\n",
    "    \n",
    "    # Save results to JSON\n",
    "    output_file = os.path.join(folder_path, 'x.json')\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    return output_file\n",
    "\n",
    "# Usage\n",
    "folder_path = \"a\"\n",
    "api_key = \"c8d9ab0b4a4b127e62f40120f887dd64.ZB5RpBTqOT5rUhtS\"\n",
    "output_file = process_images(folder_path, api_key)\n",
    "print(f\"Results saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d22dbe5-ac4d-4906-8f13-98f1cf54dc8e",
   "metadata": {},
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "raw",
   "id": "afc8f395-22e3-4a0a-965d-a98b3d6eeca9",
   "metadata": {},
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "url = \"https://api.ainewserver.com/v1/chat/completions\"\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": \"xxx\",\n",
    "    \"content-type\": \"application/json\"\n",
    "}\n",
    "data = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"hello,can your read image\",\n",
    "        }\n",
    "    ],\n",
    "    \"model\": \"gemini-2.0-flash-exp\"\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "data = json.loads(response.text)\n",
    "print(data['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8a22a5-9dcb-4642-b9dd-ab239f3124de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "data = json.loads(response.text)\n",
    "print(data['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43585af3-e95d-4a04-aa7c-8548abff5bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Unfortunately, I cannot process or analyze images directly. However, if you describe the image or provide details about it, I can certainly help interpret or provide information based on your description. Let me know how I can assist!\n"
     ]
    }
   ],
   "source": [
    "# Please install OpenAI SDK first: `pip3 install openai`\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"sk-53b8c802015f4ed795f873e74b67072a\", base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello,can you process image\"},\n",
    "    ],\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b135c63d-f4f5-42c1-92dd-df8304af32b4",
   "metadata": {},
   "source": [
    "import json\n",
    "\n",
    "# Function to determine category based on key range\n",
    "def get_category(key):\n",
    "    key_num = int(key)\n",
    "    if 1 <= key_num <= 1024:\n",
    "        return \"Public Figures\"\n",
    "    elif 1025 <= key_num <= 1692:\n",
    "        return \"Popular Media\"\n",
    "    elif 1693 <= key_num <= 2646:\n",
    "        return \"Linguistic Representations\"\n",
    "    elif 2647 <= key_num <= 3529:\n",
    "        return \"Literary Works\"\n",
    "    return \"Unknown\"\n",
    "\n",
    "# Load the input JSON file\n",
    "with open('json/result_3.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Modify the dictionary by adding the category\n",
    "for key in data:\n",
    "    category = get_category(key)\n",
    "    data[key].append(category)\n",
    "\n",
    "# Save the modified dictionary to a new JSON file\n",
    "with open('output.json', 'w') as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b0b1bcfb-6f1f-453b-8191-80f0916f4805",
   "metadata": {},
   "source": [
    "import json\n",
    "\n",
    "# Function to determine category based on key range\n",
    "def get_category(key):\n",
    "    key_num = int(key)\n",
    "    if 0 <= key_num <= 1024:\n",
    "        return [\"Which Celebrity?\"]\n",
    "    elif 1025 <= key_num <= 1067:\n",
    "        return [\"Guess The ANIME\"]\n",
    "    elif 1068 <= key_num <= 1084:\n",
    "        return [\"Guess the app\"]\n",
    "    elif 1085 <= key_num <= 1093:\n",
    "        return [\"Guess The Game\"]\n",
    "    elif 1094 <= key_num <= 1223:\n",
    "        return [\"GUESS THE MOVIE\"]\n",
    "    elif 1224 <= key_num <= 1550:\n",
    "        return [\"Which movie?\"]\n",
    "    elif 1551 <= key_num <= 1599:\n",
    "        return [\"GUESS THE MOVIE\"]\n",
    "    elif 1600 <= key_num <= 1623:\n",
    "        return [\"Guesss the song\"]\n",
    "    elif 1624 <= key_num <= 1692:\n",
    "        return [\"Which TV?\"]\n",
    "    elif 1693 <= key_num <= 2125:\n",
    "        return [\"Guess Idiom\"]\n",
    "    elif 2126 <= key_num <= 2253:\n",
    "        return [\"Guess Phrase\"]\n",
    "    elif 2254 <= key_num <= 2269:\n",
    "        return [\"Guess Proverb\"]\n",
    "    elif 2270 <= key_num <= 2297:\n",
    "        return [\"Guess Slang\"]\n",
    "    elif 2298 <= key_num <= 2646:\n",
    "        return [\"Guess the Word\"]\n",
    "    elif 2647 <= key_num <= 2736:\n",
    "        return [\"Guess the book\"]\n",
    "    elif 2737 <= key_num <= 3341:\n",
    "        return [\"Which fictional work?\"]\n",
    "    elif 3342 <= key_num <= 3529:\n",
    "        return [\"Which fictional being?\"]\n",
    "    return [\"Unknown\", \"Unknown\"]\n",
    "\n",
    "# Load the input JSON file\n",
    "with open('json/result_3.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Modify the dictionary by adding both categories\n",
    "for key in data:\n",
    "    categories = get_category(key)\n",
    "    data[key].extend(categories)\n",
    "\n",
    "# Save the modified dictionary to a new JSON file\n",
    "with open('output.json', 'w') as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a1c7bb3e-c5f0-4222-9208-f7a82654602b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2418, 1929, 2749, 1807, 1777, 1066]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Function to sum binary lists\n",
    "def sum_binary_lists(data_dict):\n",
    "    # Initialize result list with zeros\n",
    "    result = [0] * 6  # Assuming all binary lists have length 6\n",
    "    \n",
    "    # Iterate through all items in dictionary\n",
    "    for item in data_dict.values():\n",
    "        binary_list = item[1]  # Get the binary list (second element)\n",
    "        # Sum corresponding elements\n",
    "        for i in range(len(binary_list)):\n",
    "            result[i] += binary_list[i]\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Read from JSON file\n",
    "with open('json/result_3.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Calculate sum\n",
    "result = sum_binary_lists(data)\n",
    "print(result)\n",
    "# Write result to JSON file\n",
    "# with open('output.json', 'w') as f:\n",
    "#     json.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ef748722-50a5-42a9-9789-52156dfc03a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[285, 436, 516, 555, 586, 637, 514]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def count_third_values(data_dict):\n",
    "    # Initialize result list with zeros for numbers 0-6 (7 positions)\n",
    "    result = [0] * 7\n",
    "    \n",
    "    # Count frequency of each number\n",
    "    for item in data_dict.values():\n",
    "        third_value = item[2]  # Get the third value\n",
    "        result[third_value] += 1\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Read from JSON file\n",
    "with open('json/result_3.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Calculate frequency\n",
    "result = count_third_values(data)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cd2cb1-bb6c-4ed8-9d48-060aa140df7e",
   "metadata": {},
   "source": [
    "number of  0 to 6 recognitions:[285, 436, 516, 555, 586, 637, 514]. from this result ,we can further look into following interetsting points:\n",
    "- for the number of 0 recognition(285),which means the number of images that all the LLMs fails to recognise, so when evaluating the recognition performance, it's better to evlauate against the number of at least one recognition(3529-285).\n",
    "- for the total number of 1 recogintion(436),we can further look into ,each LLM contribute how much of these number.\n",
    "- for the total number of 5 recognition(637),another interesting point is ,each LLM contribute how much to this number, which means, each LLM occupy how much percentage of in being the only one LLM that failed the specific task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6781e2-5938-4d69-862b-0c181aa946a0",
   "metadata": {},
   "source": [
    "## print out the presentation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e4237871-17c2-4608-af0d-6091bb4dc3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = \"\"\"\n",
    "Analyze visual representations of literature works and fictional characters. For each visual representation, identify which of these methods is used:\n",
    "\n",
    "1. Illustration - Detailed artistic depiction of scenes, characters, or moments from literature works\n",
    "2. Cartoon - Simplified or exaggerated drawing style for humorous effect\n",
    "3. Caricature - Intentionally distorted features of characters for comic or satirical purposes\n",
    "4. Emoji Combination - Multiple emojis merged to represent literary elements\n",
    "5. Minimalism - Simplified visual representation using minimal design elements\n",
    "6. Other - If none above, specify the visual method used\n",
    "\n",
    "Rules:\n",
    "- Choose only ONE method per visual representation\n",
    "- If multiple methods seem applicable, select the one that appears earlier in the list\n",
    "- For methods 1-5, respond with just the method name\n",
    "- For method 6 (Other), provide the specific visual/artistic term that describes the method used\n",
    "\n",
    "Example scenarios:\n",
    "- A detailed painting of Alice falling down the rabbit hole ‚Üí Illustration\n",
    "- A whimsical, simplified drawing of Don Quixote fighting windmills ‚Üí Cartoon\n",
    "- An exaggerated sketch of Captain Ahab's obsessive expression ‚Üí Caricature\n",
    "- üê∫ + üëµ to represent Little Red Riding Hood ‚Üí Emoji Combination\n",
    "- Single line drawing to represent Odysseus's journey ‚Üí Minimalism\n",
    "- Abstract shapes representing the transformation in Kafka's Metamorphosis ‚Üí Other (Abstract Symbolism)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "ecc1f367-464f-47de-a3b9-bde48aed0e3c",
   "metadata": {},
   "source": [
    "import os\n",
    "import base64\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import httpx\n",
    "import requests\n",
    "\n",
    "def analyze_representations(data_file, img_folder):\n",
    "    client = OpenAI(\n",
    "        base_url=\"https://api.xty.app/v1\", \n",
    "        api_key=\"xxxx\",\n",
    "        http_client=httpx.Client(\n",
    "            base_url=\"https://api.xty.app/v1\",\n",
    "            follow_redirects=True,\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    # Load the data pairs\n",
    "    with open(data_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Create output dictionary\n",
    "    results = {}\n",
    "    \n",
    "    for key, value in data.items():\n",
    "        img_file = f\"{key}.jpg\"\n",
    "        img_path = os.path.join(img_folder, img_file)\n",
    "        \n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"Image not found: {img_file}\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Read and encode image\n",
    "            with open(img_path, 'rb') as f:\n",
    "                img_base = base64.b64encode(f.read()).decode('utf-8')\n",
    "                img_base = f\"data:image/jpeg;base64,{img_base}\"\n",
    "                \n",
    "            prompt = pre + f\"\\nImage name: {img_file}\\nPublic Figure: {value[0]}\"\n",
    "\n",
    "            completion = client.chat.completions.create(\n",
    "                model=\"gemini-2.0-flash-exp\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\":[\n",
    "                        {\"type\": \"image_url\", \"image_url\": {\"url\": img_base}},\n",
    "                        {\"type\": \"text\", \"text\": prompt}\n",
    "                    ]}\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            method = completion.choices[0].message.content\n",
    "            results[key] = method\n",
    "            \n",
    "            print(f\"Processed {key} | {value[0]} | Method: {method}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_file}: {str(e)}\")\n",
    "            results[key] = \"error\"\n",
    "    \n",
    "    # Save results\n",
    "    output_file = \"representation_analysis4.json\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    return output_file\n",
    "\n",
    "# Usage\n",
    "data_file = \"four/r4.json\"  # Your data pairs file\n",
    "img_folder = \"img\"          # Folder containing the images\n",
    "output_file = analyze_representations(data_file, img_folder)\n",
    "print(f\"Results saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "74e105fa-e2cb-4377-ba37-431d5ccc569d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af24dfab-84a7-4a8e-8884-aee576a130ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
